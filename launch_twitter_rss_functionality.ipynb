{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "import os, re\n",
    "import twint\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "from newspaper import Article\n",
    "import numpy as np\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import image_utils as img_utils\n",
    "import twitter_marketing_funcs as twt_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBuild functionality that gets the tweets\\n- Actually for the RSS, only use content where the videos have been generated on Bloverse. This would potentially make life easier for us\\n- So what would happen is that we only generate the comment videos for articles that have already been generated for RSS\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build functionality that gets the tweets\n",
    "- Actually for the RSS, only use content where the videos have been generated on Bloverse. This would potentially make life easier for us\n",
    "- So what would happen is that we only generate the comment videos for articles that have already been generated for RSS\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-26 16:00:00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 1 - Get the date for today and 7 days ago\n",
    "\"\"\"\n",
    "time_diff = 1 # This means that we want to get tweets that were posted in the last hour. In production we will probably go for 3\n",
    "today = datetime.now() \n",
    "\n",
    "date = today.date()\n",
    "hour = max(0, today.hour - time_diff)\n",
    "minute = '00'\n",
    "second = '00'\n",
    "today_date_str = '%s %s:%s:%s' % (date, hour, minute, second)\n",
    "print(today_date_str)\n",
    "\n",
    "num_tweets = 5000 # The maximum number of tweets that would be extracted in any given run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\USER\\\\Desktop\\\\NEW_DEMZ\\\\bloverse'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Unnamed: 0.1            Name Content Type Twitter Handle  \\\n",
      "0           0             0             CNN      Country            cnn   \n",
      "1           1             1        Fox News      Country        foxnews   \n",
      "2           2             2        CBS News      Country        cbsnews   \n",
      "3           3             3        NBC News      Country        nbcnews   \n",
      "4           4             4  Breitbart News      Country  breitbartnews   \n",
      "\n",
      "                                                 Bio  \\\n",
      "0  Itâ€™s our job to #GoThere & tell the most diffi...   \n",
      "1  Follow America's #1 cable news network, delive...   \n",
      "2  Your source for original reporting and trusted...   \n",
      "3  News updates from around the ðŸŒŽ, all day, every...   \n",
      "4  News, commentary, and destruction of the polit...   \n",
      "\n",
      "                                      Logo Image URL Primary Colour  \\\n",
      "0  https://pbs.twimg.com/profile_images/127825916...        #cd1316   \n",
      "1  https://pbs.twimg.com/profile_images/918480715...        #fbffff   \n",
      "2  https://pbs.twimg.com/profile_images/645966750...        #151618   \n",
      "3  https://pbs.twimg.com/profile_images/133849791...        #102039   \n",
      "4  https://pbs.twimg.com/profile_images/949270171...        #fffeff   \n",
      "\n",
      "  Secondary Colour                      URL      Location  Twitter Followers  \\\n",
      "0          #ffffff   http://t.co/IaghNW8Xm2           NaN           53167195   \n",
      "1          #000000   http://t.co/ZYG58XZtAC        U.S.A.           20116931   \n",
      "2          #ffffff  https://t.co/VGut7r2Vg5  New York, NY            8080387   \n",
      "3          #ffffff  https://t.co/Z73is4fJ3x  New York, NY            8388112   \n",
      "4          #000000   http://t.co/2sVbt3n6lO           NaN            1491078   \n",
      "\n",
      "   Twitter Following  Verified       Category  Avg Daily Tweets  \\\n",
      "0               1100      True  United States                84   \n",
      "1                258      True  United States                26   \n",
      "2                564      True  United States                94   \n",
      "3               1847      True  United States               117   \n",
      "4                111      True  United States                45   \n",
      "\n",
      "   Avg Daily Article Tweets  Avg Daily Comments  Comment Tweet Percentage  \\\n",
      "0                        81               10304                        96   \n",
      "1                        26                6685                       100   \n",
      "2                        80                4908                        84   \n",
      "3                       103                4766                        87   \n",
      "4                        44                4369                        96   \n",
      "\n",
      "   Allocated Num Comments  \n",
      "0                      70  \n",
      "1                      45  \n",
      "2                      35  \n",
      "3                      35  \n",
      "4                      30  \n"
     ]
    }
   ],
   "source": [
    "rss_twitter_handles_path = 'C:\\\\Users\\\\USER\\\\Desktop\\\\NEW_DEMZ\\\\bloverse\\\\rss_publications_final.csv'\n",
    "rss_publication_df = pd.read_csv(rss_twitter_handles_path)\n",
    "print(rss_publication_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4706\n"
     ]
    }
   ],
   "source": [
    "## Get expected number of tweets daily\n",
    "expected_daily_tweets = sum(list(rss_publication_df['Avg Daily Tweets']))\n",
    "print(expected_daily_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "9\n",
      "\n",
      "foxnews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\desktop\\new_demz\\bloverse\\bloverseenv\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "2\n",
      "\n",
      "cbsnews\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "8\n",
      "\n",
      "nbcnews\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "4\n",
      "\n",
      "breitbartnews\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "4\n",
      "\n",
      "bbcnews\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "5\n",
      "\n",
      "bbcworld\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "2\n",
      "\n",
      "dailymailuk\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "15\n",
      "\n",
      "guardian\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "17\n",
      "\n",
      "independent\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "41\n",
      "\n",
      "saharareporters\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "3\n",
      "\n",
      "mobilepunch\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "8\n",
      "\n",
      "vanguardngrnews\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "40\n",
      "\n",
      "thecableng\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "7\n",
      "\n",
      "gazettengr\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "1\n",
      "\n",
      "nationafrica\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "14\n",
      "\n",
      "standardkenya\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "3\n",
      "\n",
      "thestarkenya\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "14\n",
      "\n",
      "timesofindia\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "11\n",
      "\n",
      "indianexpress\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "5\n",
      "\n",
      "the_hindu\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "10\n",
      "\n",
      "thequint\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "11\n",
      "\n",
      "pulseghana\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "12\n",
      "\n",
      "ctvnews\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "23\n",
      "\n",
      "torontostar\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "3\n",
      "\n",
      "nationalpost\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "9\n",
      "\n",
      "globalnews\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "4\n",
      "\n",
      "citynews\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "4\n",
      "\n",
      "verge\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "3\n",
      "\n",
      "wired\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "5\n",
      "\n",
      "gizmodo\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "3\n",
      "\n",
      "entrepreneur\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "5\n",
      "\n",
      "bbctech\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "\"None of [Index(['id', 'conversation_id', 'date', 'tweet', 'language', 'hashtags',\\n       'username', 'name', 'link', 'urls', 'photos', 'video', 'thumbnail',\\n       'retweet', 'nlikes', 'nreplies', 'nretweets', 'source'],\\n      dtype='object')] are in the [columns]\"\n",
      "0\n",
      "\n",
      "skysportspl\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "3\n",
      "\n",
      "fourfourtwo\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "1\n",
      "\n",
      "skycricket\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "7\n",
      "\n",
      "thr\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "5\n",
      "\n",
      "movieweb\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "6\n",
      "\n",
      "comingsoonnet\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "3\n",
      "\n",
      "webmd\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "3\n",
      "\n",
      "bbchealth\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "4\n",
      "\n",
      "tmz\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "12\n",
      "\n",
      "people\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "17\n",
      "\n",
      "bollyhungama\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "4\n",
      "\n",
      "hollywoodlife\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "4\n",
      "\n",
      "business\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "46\n",
      "\n",
      "wsj\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "6\n",
      "\n",
      "theeconomist\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "12\n",
      "\n",
      "marketwatch\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "9\n",
      "\n",
      "ft\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latest_tweet_df_list = []\n",
    "for i in range(len(rss_publication_df)):\n",
    "    rss_twitter_handle = rss_publication_df['Twitter Handle'].iloc[i]\n",
    "    print(rss_twitter_handle)\n",
    "    tweet_df = twt_mark.get_latest_tweets_from_handle(rss_twitter_handle, num_tweets, today_date_str)\n",
    "    tweet_df['Twitter Handle'] = rss_twitter_handle\n",
    "    print(len(tweet_df))\n",
    "    latest_tweet_df_list.append(tweet_df)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest_tweet_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_tweets_df = pd.concat(latest_tweet_df_list, sort=True)\n",
    "\n",
    "tweet_w_link_indices = []\n",
    "link_tweet_w_comment_indices = []\n",
    "for ii in range(len(latest_tweets_df)):\n",
    "    tweet_text = latest_tweets_df.iloc[ii]['tweet']\n",
    "    tweet_urls = latest_tweets_df.iloc[ii]['urls']\n",
    "    num_tweet_comments = latest_tweets_df.iloc[ii]['nreplies']\n",
    "    if len(tweet_urls) > 0:\n",
    "#         print(tweet_text)\n",
    "#         print(tweet_urls)\n",
    "#         print()\n",
    "        tweet_w_link_indices.append(ii)\n",
    "        if num_tweet_comments > 1:\n",
    "            link_tweet_w_comment_indices.append(ii)\n",
    "\n",
    "link_tweets_df = latest_tweets_df.iloc[tweet_w_link_indices]\n",
    "comment_tweets_df = latest_tweets_df.iloc[link_tweet_w_comment_indices]\n",
    "total_comments = sum(list(comment_tweets_df['nreplies']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSave the link_tweets from the latest run to the db for the RSS generation\\n- Add code in here at this point to filter out any articles that have something missing and as such wont generate videos\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save the link_tweets from the latest run to the db for the RSS generation\n",
    "- Add code in here at this point to filter out any articles that have something missing and as such wont generate videos\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build algorithm that allocates the tweet ids for where we want to go and get comments from, as well as how many comments we should expect to get\n",
    "- For the latest extraction, for each handle\n",
    "1 - get a df of their latest tweets\n",
    "2 - rank the tweets by n_relies\n",
    "3 - for the top 5 tweets by replies, store their \n",
    "\"\"\"\n",
    "## Assign the target number of comments we want to extract\n",
    "target_num_comments = 500 \n",
    "# we should be running this every 3 hours from 6am UK to 6pm and build an algorithm that recommends the exactly the tweet ids for where\n",
    "# Ukeme will be going to get comments from, as well as how many comments he is expected to be getting from each tweetid\n",
    "\n",
    "## Now loop through each publication and calculate a weight \n",
    "publication_comment_tweet_ids = []\n",
    "publication_names = []\n",
    "publisher_num_comments_list = []\n",
    "publication_handle_list = list(rss_publication_df['Twitter Handle'])\n",
    "total_comments = sum(list(rss_publication_df['Avg Daily Comments']))\n",
    "\n",
    "## To make it simple, for each handle, just get 5 comments from each run\n",
    "## This also adds a bit of fairness to proceedings\n",
    "for publication in publication_handle_list:\n",
    "    comment_tweet_id_list = []\n",
    "    publication_info = rss_publication_df[rss_publication_df['Twitter Handle']==publication]\n",
    "    target_num_tweets = 5\n",
    "    try:\n",
    "        publication_df = comment_tweets_df[comment_tweets_df['Twitter Handle']==publication]\n",
    "        publication_df = publication_df.sort_values(by=['nreplies'], ascending=False)\n",
    "        comment_tweet_ids = list(publication_df.iloc[0:5]['id'])\n",
    "        counter = 0\n",
    "        for i in range(len(publication_df)):\n",
    "            tweet_text = publication_df.iloc[i]['tweet']\n",
    "            tweet_id = publication_df.iloc[i]['id']\n",
    "            num_comments = publication_df.iloc[i]['nreplies']\n",
    "            if num_comments > 5:\n",
    "                comment_tweet_id_list.append(tweet_id)\n",
    "                counter += 1\n",
    "                if counter >= 5:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "#         print(e)\n",
    "        pass\n",
    "    if len(comment_tweet_id_list) > 0:\n",
    "        publication_names.append(publication)\n",
    "        publication_comment_tweet_ids.append(comment_tweet_id_list)\n",
    "publication_tweet_comment_search_dict = dict(zip(publication_names, publication_comment_tweet_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cnn': ['1375468096798998529', '1375479438402523140', '1375473906681794560', '1375471913204596745', '1375477698143916032'], 'foxnews': ['1375469079377686533', '1375476759957766145'], 'cbsnews': ['1375462548254949382', '1375477648030363658', '1375473754969681923', '1375472500428189697', '1375481374283608068'], 'nbcnews': ['1375462663594053632', '1375480032299208704', '1375469707789238278', '1375482944568107009'], 'breitbartnews': ['1375471632320446469', '1375477147578605576', '1375466089413496832', '1375482572239728643'], 'bbcnews': ['1375470342655852549', '1375475422301937668', '1375477414814380038', '1375481918695862274', '1375465087436210186'], 'bbcworld': ['1375478076965072897', '1375465426734489602'], 'dailymailuk': ['1375470156533600259', '1375466089006632967', '1375476193579896834', '1375482920706670593', '1375462568559534084'], 'guardian': ['1375463217410015234', '1375481332558618626', '1375474088379088902', '1375464631838326784', '1375469843151921158'], 'independent': ['1375472138132541445', '1375471166115172354', '1375464045004914688', '1375472922291269634', '1375475415586865155'], 'saharareporters': ['1375478722069983243', '1375474516760141839'], 'vanguardngrnews': ['1375480601583755264', '1375474931178295298', '1375468530938810373'], 'thecableng': ['1375475379847229450'], 'standardkenya': ['1375465485681229829'], 'timesofindia': ['1375475451846488069', '1375482297877598208', '1375469375948488712', '1375479634632970243'], 'the_hindu': ['1375465081652269056'], 'pulseghana': ['1375475426257145860'], 'ctvnews': ['1375475883264335878', '1375478386391453706'], 'torontostar': ['1375470612630499332'], 'nationalpost': ['1375476912823406596', '1375472649984425995', '1375477247994433543'], 'globalnews': ['1375464943051505669'], 'citynews': ['1375464679334674436'], 'verge': ['1375470964364955653'], 'wired': ['1375463227333689349'], 'gizmodo': ['1375474009463083010'], 'skycricket': ['1375468356422221833', '1375466592260227073', '1375477300695871490', '1375472048110178310'], 'movieweb': ['1375478171009605640'], 'tmz': ['1375477716640743424', '1375470568636579843', '1375479922316042243', '1375474766853722118', '1375480174570045444'], 'people': ['1375475680599805955', '1375471650733457411'], 'business': ['1375462824705675269', '1375463578816364546', '1375472674940538884', '1375465588793999361', '1375479330172776451'], 'wsj': ['1375463842751332357', '1375466359778344963', '1375477706633150465'], 'theeconomist': ['1375463831317712900', '1375477701058904069', '1375467870168174593'], 'marketwatch': ['1375476422886703112', '1375472593256337412']}\n"
     ]
    }
   ],
   "source": [
    "print(publication_tweet_comment_search_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBuild a functionaly that will go search for the tweets with the publication ids(publication_tweet_comment_search_dict) \\nFor each id, get 3 comments, as well as store metrics like num_likes etc etc\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build a functionaly that will go search for the tweets with the publication ids(publication_tweet_comment_search_dict) \n",
    "For each id, get 3 comments, as well as store metrics like num_likes etc etc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dates for the tweets search (since today, until tomorrow)\n",
    "# today = date.today()\n",
    "# tomorrow = date.today() + timedelta(days=1)\n",
    "\n",
    "# tomorrow = tomorrow.strftime('%Y-%m-%d')\n",
    "# today = today.strftime('%Y-%m-%d')\n",
    "# # print(tomorrow, today)\n",
    "\n",
    "time_diff = 3 # This means that we want to get tweets that were posted in the last hour. In production we will probably go for 3\n",
    "today = datetime.now() \n",
    "date = today.date()\n",
    "since_hour = max(0, today.hour - time_diff)\n",
    "until_hour = today.hour+1\n",
    "minute = '00'\n",
    "second = '00'\n",
    "since_date_str = '%s %s:%s:%s' % (date, since_hour, minute, second)\n",
    "until_date_str = '%s %s:%s:%s' % (date, until_hour, minute, second)\n",
    "# print(since_date_str)\n",
    "# print(until_date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replies_from_tweet(handle, today):\n",
    "    \"\"\"\n",
    "    This functionality get all tweets from a handle given a specific date\n",
    "    \"\"\"\n",
    "    c = twint.Config()\n",
    "    c.Since = today\n",
    "#     c.Until = tomorrow\n",
    "    c.Pandas = True\n",
    "    c.To = handle #the handle of the twitter account\n",
    "    c.Hide_output = True\n",
    "    c.Limit = 2500\n",
    "    c.Store_csv = True\n",
    "    twint.run.Search(c)\n",
    "    replies_df = twint.storage.panda.Tweets_df\n",
    "\n",
    "    return replies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_top_3_comments_of_tweet_df(replies_df, tweet_id):\n",
    "\n",
    "    new_df = replies_df[replies_df['conversation_id']==tweet_id]\n",
    "    new_df = new_df.sort_values(['nlikes'], ascending=[False]).head(3)\n",
    "    #n_likes = new_df['nlikes']\n",
    "\n",
    "    comments = new_df['tweet']\n",
    "    nlikes = new_df['nlikes']\n",
    "    _id = new_df['id']\n",
    "    nreplies = new_df['nreplies']\n",
    "\n",
    "    comments = [re.sub(r'^@\\w+',\"\", a) for a in comments]\n",
    "\n",
    "    comments = [a.lstrip() for a in comments]\n",
    "\n",
    "    new_top_3_comments_df = pd.DataFrame(columns=['id', 'username', 'comments', 'nlikes', 'nreplies', 'nretweets'])\n",
    "\n",
    "    for i in range(len(new_df)):\n",
    "        id = new_df.iloc[i]['id']\n",
    "        username = new_df.iloc[i]['username']\n",
    "        comment = new_df.iloc[i]['tweet'] \n",
    "        nlike = new_df.iloc[i]['nlikes']\n",
    "        nreply = new_df.iloc[i]['nreplies']\n",
    "        nretweet = new_df.iloc[i]['nretweets']\n",
    "\n",
    "        comment = re.sub(r'^@\\w+',\"\", comment).lstrip() # remove the @ at the beginning of the comments\n",
    "\n",
    "        comments_metrics = [id, username, comment, nlike, nreply, nretweet]\n",
    "        new_top_3_comments_df.loc[i] = comments_metrics\n",
    "\n",
    "    return new_top_3_comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweet_id = \"1375468096798998529\"\n",
    "\n",
    "replies_df = get_replies_from_tweet(\"cnn\", since_date_str)\n",
    "new_top_3_comments_df = get_top_3_comments_of_tweet_df(replies_df, tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>comments</th>\n",
       "      <th>nlikes</th>\n",
       "      <th>nreplies</th>\n",
       "      <th>nretweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1375469217475088391</td>\n",
       "      <td>Professing_Prof</td>\n",
       "      <td>America: Where you can give a gun, but not wat...</td>\n",
       "      <td>208</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1375468484843409408</td>\n",
       "      <td>the_livelihood</td>\n",
       "      <td>No one deserves a birth right to own this kind...</td>\n",
       "      <td>124</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1375469206141997056</td>\n",
       "      <td>PaigePaulston</td>\n",
       "      <td>Seriously, when are we going to learn? We NEED...</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id         username  \\\n",
       "0  1375469217475088391  Professing_Prof   \n",
       "1  1375468484843409408   the_livelihood   \n",
       "2  1375469206141997056    PaigePaulston   \n",
       "\n",
       "                                            comments nlikes nreplies nretweets  \n",
       "0  America: Where you can give a gun, but not wat...    208       17        11  \n",
       "1  No one deserves a birth right to own this kind...    124       10         3  \n",
       "2  Seriously, when are we going to learn? We NEED...     60       10         4  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_top_3_comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replies_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For Bruno Part 1\n",
    "Next build a function that :\n",
    "- [Done] Goes through latest tweets for all publication handles and then gets the tweet ids for the top 5 tweets with comments\n",
    "- [Done] Return a dictionary of tweet handle and tweet id for where we will be getting comments\n",
    "- For each id, get 3 comments, as well as store metrics like num_likes etc etc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For Bruno Part 2\n",
    "- Next also build the function that identifies the latest tweet threads that we can use for Bloverse stories\n",
    "- ** FOr the medium, lbogger and subbstack strategies, add functionality where we also automatically get the twitter details of the user who created the article\n",
    "- Also build the function for the blue tick strategy, this is likely one that Halima will have to personally handle given she has better context, what we can do though\n",
    "is to identify the nigerian, ghanain and indian blue ticks and then leave those to the respective marketing hands to handle\n",
    "- Then finally, the algorithm for VC twitter handles, and then getting their latest content that we can push out as Bloverse stories... However lets verify the twitter handles are legit VCs first.\n",
    "- Also work on the 'trending detection algorithm' for the RSS tweets so users can always get a view of the content/articles that are trending\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
